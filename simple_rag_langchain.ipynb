{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG (Retrieval-Augmented Generation) Implementation with LangChain 1.0+\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete RAG pipeline using **LangChain 1.0+ with LCEL** (LangChain Expression Language).\n",
    "\n",
    "### What is RAG?\n",
    "RAG combines retrieval of relevant documents with generation from a Large Language Model (LLM):\n",
    "1. **Retrieval**: Find relevant information from a knowledge base\n",
    "2. **Augmentation**: Add retrieved context to the prompt\n",
    "3. **Generation**: LLM generates answers based on the context\n",
    "\n",
    "### Pipeline Flow:\n",
    "```\n",
    "PDF Documents → Load → Split into Chunks → Create Embeddings → Store in Vector DB\n",
    "                                                                         ↓\n",
    "User Query → Retrieve Similar Chunks → Combine with Query → LLM → Answer\n",
    "```\n",
    "\n",
    "### Components Used:\n",
    "- **Document Loader**: PyPDFLoader (for PDF processing)\n",
    "- **Text Splitter**: RecursiveCharacterTextSplitter (smart chunking)\n",
    "- **Embeddings**: OpenAI text-embedding-3-small (vector representations)\n",
    "- **Vector Store**: FAISS (fast similarity search)\n",
    "- **LLM**: OpenAI GPT-4-Turbo or GPT-3.5-Turbo\n",
    "- **Chain Builder**: LCEL (LangChain Expression Language)\n",
    "\n",
    "### LangChain 1.0+ Features:\n",
    "- ✅ Modern LCEL syntax with pipe operator `|`\n",
    "- ✅ More readable and composable chains\n",
    "- ✅ Better streaming support\n",
    "- ✅ Type-safe operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, install all required packages. Make sure you have Python 3.9+ installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.7-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.7/93.7 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
      "     -------------------------------------- 471.5/471.5 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.5/82.5 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl (18.2 MB)\n",
      "     ---------------------------------------- 18.2/18.2 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
      "     -------------------------------------- 326.6/326.6 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv in c:\\python311\\lib\\site-packages (1.1.1)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "     -------------------------------------- 879.4/879.4 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting notebook\n",
      "  Downloading notebook-7.4.7-py3-none-any.whl (14.3 MB)\n",
      "     ---------------------------------------- 14.3/14.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting langgraph<1.1.0,>=1.0.2\n",
      "  Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
      "     -------------------------------------- 156.8/156.8 kB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\python311\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\python311\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Collecting langsmith<1.0.0,>=0.3.45\n",
      "  Downloading langsmith-0.4.43-py3-none-any.whl (410 kB)\n",
      "     -------------------------------------- 410.2/410.2 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\python311\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\python311\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\python311\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\python311\\lib\\site-packages (from langchain-openai) (2.1.0)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0\n",
      "  Downloading sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\python311\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "     -------------------------------------- 456.2/456.2 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "     ---------------------------------------- 51.9/51.9 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\python311\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.7/277.7 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting nbconvert\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "     -------------------------------------- 258.5/258.5 kB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipykernel in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter) (6.30.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.8/139.8 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting jupyterlab\n",
      "  Downloading jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
      "     ---------------------------------------- 12.3/12.3 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Downloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "     -------------------------------------- 388.2/388.2 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting jupyterlab-server<3,>=2.27.1\n",
      "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.8/59.8 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting notebook-shim<0.3,>=0.2\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from notebook) (6.5.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.6/41.6 kB ? eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.9/86.9 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python311\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\python311\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.11.0)\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\python311\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.8.1)\n",
      "Collecting jupyter-events>=0.11.0\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting overrides>=5.0\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting prometheus-client>=0.9\n",
      "  Downloading prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.1/61.1 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting pywinpty>=2.0.1\n",
      "  Downloading pywinpty-3.0.2-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (27.1.0)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\python311\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.9.0)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\python311\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 76.7/76.7 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\python311\\lib\\site-packages (from jupyterlab->jupyter) (65.5.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (9.6.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Collecting babel>=2.10\n",
      "  Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "     ---------------------------------------- 10.2/10.2 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting json5>=0.9.0\n",
      "  Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Collecting jsonschema>=4.18.0\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.2/46.2 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2\n",
      "  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\python311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\python311\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\python311\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.4/106.4 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Downloading bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "     -------------------------------------- 164.4/164.4 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\python311\\lib\\site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.5/53.5 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from nbconvert->jupyter) (2.19.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "     -------------------------------------- 299.1/299.1 kB 6.1 MB/s eta 0:00:00\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "     -------------------------------------- 914.9/914.9 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-console->jupyter) (3.0.52)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python311\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.28.0-cp311-cp311-win_amd64.whl (223 kB)\n",
      "     -------------------------------------- 224.0/224.0 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (311)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting ormsgpack>=1.12.0\n",
      "  Downloading ormsgpack-1.12.0-cp311-cp311-win_amd64.whl (112 kB)\n",
      "     -------------------------------------- 112.7/112.7 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.14)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.5)\n",
      "Collecting fqdn\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting uri-template\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Downloading webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.23)\n",
      "Collecting lark>=1.2.2\n",
      "  Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.2/113.2 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting arrow>=0.15.0\n",
      "  Downloading arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "     ---------------------------------------- 68.8/68.8 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tzdata in c:\\python311\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2025.2)\n",
      "Installing collected packages: webencodings, fastjsonschema, widgetsnbextension, webcolors, uri-template, tinycss2, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, regex, pywinpty, python-json-logger, pypdf, propcache, prometheus-client, pandocfilters, overrides, ormsgpack, mypy-extensions, multidict, mistune, marshmallow, lark, jupyterlab_widgets, jupyterlab-pygments, json5, httpx-sse, greenlet, frozenlist, fqdn, faiss-cpu, defusedxml, bleach, babel, async-lru, aiohappyeyeballs, yarl, typing-inspect, tiktoken, terminado, SQLAlchemy, rfc3987-syntax, requests-toolbelt, referencing, beautifulsoup4, arrow, argon2-cffi-bindings, aiosignal, pydantic-settings, langsmith, langgraph-sdk, jupyter-server-terminals, jsonschema-specifications, isoduration, ipywidgets, dataclasses-json, argon2-cffi, aiohttp, langchain-core, jupyter-console, jsonschema, nbformat, langgraph-checkpoint, langchain-text-splitters, langchain-openai, nbclient, langgraph-prebuilt, langchain-classic, jupyter-events, nbconvert, langgraph, langchain-community, langchain, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python311\\\\etc\\\\jupyter\\\\nbconfig\\\\notebook.d'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run ONE of the following options:\n",
    "\n",
    "# Option 1: Install from requirements.txt (RECOMMENDED)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Option 2: Install all packages individually\n",
    "# !pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken jupyter notebook\n",
    "\n",
    "# Option 3: Quick install (if you're having import issues)\n",
    "# !pip install --upgrade langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary modules with explanations of what each does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "If you encounter import errors, run this cell first to check package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ langchain: 1.0.5\n",
      "✓ langchain-core: 1.0.5\n",
      "✗ langchain-openai not installed\n",
      "✓ langchain-community: 0.4.1\n",
      "\n",
      "Python version: 3.10.18 (main, Sep 18 2025, 19:42:01) [MSC v.1944 64 bit (AMD64)]\n",
      "\n",
      "If any packages are missing, run:\n",
      "pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\n"
     ]
    }
   ],
   "source": [
    "# Check installed package versions\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"✓ langchain: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(f\"✓ langchain-core: {langchain_core.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain-core not installed - REQUIRED!\")\n",
    "    print(\"  Run: pip install langchain-core\")\n",
    "\n",
    "try:\n",
    "    import langchain_openai\n",
    "    print(f\"✓ langchain-openai: {langchain_openai.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain-openai not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_community\n",
    "    print(f\"✓ langchain-community: {langchain_community.__version__}\")\n",
    "except:\n",
    "    print(\"✗ langchain-community not installed\")\n",
    "\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(\"\\nIf any packages are missing, run:\")\n",
    "print(\"pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-1.0.7-py3-none-any.whl (93 kB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl (18.2 MB)\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\python311\\lib\\site-packages (1.1.1)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2\n",
      "  Using cached langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\python311\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\python311\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Collecting langsmith<1.0.0,>=0.3.45\n",
      "  Using cached langsmith-0.4.43-py3-none-any.whl (410 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\python311\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\python311\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\python311\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\python311\\lib\\site-packages (from langchain-openai) (2.1.0)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0\n",
      "  Using cached sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\python311\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\python311\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python311\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2\n",
      "  Using cached langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\python311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python311\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\python311\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\python311\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python311\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Collecting ormsgpack>=1.12.0\n",
      "  Using cached ormsgpack-1.12.0-cp311-cp311-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\simil\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: regex, pypdf, propcache, ormsgpack, mypy-extensions, multidict, marshmallow, httpx-sse, greenlet, frozenlist, faiss-cpu, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, aiosignal, pydantic-settings, langsmith, langgraph-sdk, dataclasses-json, aiohttp, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain\n",
      "  Attempting uninstall: ormsgpack\n",
      "    Found existing installation: ormsgpack 1.11.0\n",
      "    Uninstalling ormsgpack-1.11.0:\n",
      "      Successfully uninstalled ormsgpack-1.11.0\n",
      "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 dataclasses-json-0.6.7 faiss-cpu-1.12.0 frozenlist-1.8.0 greenlet-3.2.4 httpx-sse-0.4.3 langchain-1.0.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.5 langchain-openai-1.0.3 langchain-text-splitters-1.0.0 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.4 langgraph-sdk-0.2.9 langsmith-0.4.43 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 ormsgpack-1.12.0 propcache-0.4.1 pydantic-settings-2.12.0 pypdf-6.2.0 regex-2025.11.3 requests-toolbelt-1.0.0 tiktoken-0.12.0 typing-inspect-0.9.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ecomm-prod-assistant 0.1.0 requires streamlit==1.49.1, which is not installed.\n",
      "ecomm-prod-assistant 0.1.0 requires langchain==0.3.27, but you have langchain 1.0.7 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ Compatible with LangChain 1.0+\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variable management - for secure API key handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Document Loaders - for loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters - for breaking documents into manageable chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAI Integration - for embeddings and LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Vector Store - FAISS for efficient similarity search\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(\"✓ Compatible with LangChain 1.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "### Setting up OpenAI API Key\n",
    "\n",
    "You have two options:\n",
    "1. **Recommended**: Create a `.env` file with `OPENAI_API_KEY=your_key_here`\n",
    "2. **Alternative**: Set it directly in code (not recommended for production)\n",
    "\n",
    "Get your API key from: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Key loaded successfully!\n",
      "✓ Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or uncomment the line below:\")\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "else:\n",
    "    print(\"✓ OpenAI API Key loaded successfully!\")\n",
    "    print(f\"✓ Key starts with: {os.getenv('OPENAI_API_KEY')[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Loading\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "PyPDFLoader extracts text from PDF files page by page. Each page becomes a separate document with metadata (page number, source file).\n",
    "\n",
    "**How it works:**\n",
    "- Reads PDF files and extracts text content\n",
    "- Preserves page numbers for source tracking\n",
    "- Returns Document objects with `.page_content` and `.metadata`\n",
    "\n",
    "**Note**: Update the `pdf_path` variable to point to your PDF file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 15 pages from 'attention.pdf'\n",
      "\n",
      "--- First Document Preview ---\n",
      "Content (first 500 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz ...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters across all pages: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"⚠️  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF\n",
    "    # Each page becomes a separate Document object\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information about loaded documents\n",
    "    print(f\"✓ Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Content (first 500 chars): {documents[0].page_content[:500]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters across all pages: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple PDFs (Optional)\n",
    "\n",
    "If you have multiple PDF files, you can load them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files\n",
      "  ✓ Loaded 19 pages from rag.pdf\n",
      "  ✓ Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 40\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading multiple PDFs from a directory\n",
    "# Uncomment and modify if you want to load multiple files\n",
    "\n",
    "pdf_directory = \"./pdfs\"  # Directory containing your PDFs\n",
    "all_documents = []\n",
    "\n",
    "if os.path.exists(pdf_directory):\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  ✓ Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Splitting\n",
    "\n",
    "### Why Split Documents?\n",
    "- LLMs have token limits (e.g., 4K, 8K, 128K tokens)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Balance: chunks must be large enough to contain meaningful context but small enough to be specific\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "This splitter tries to keep related text together by recursively splitting on:\n",
    "1. Paragraphs (`\\n\\n`)\n",
    "2. Lines (`\\n`)\n",
    "3. Sentences (`. `)\n",
    "4. Words (` `)\n",
    "5. Characters (as last resort)\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=1024`: Target size for each chunk (in characters)\n",
    "- `chunk_overlap=128`: Overlap between chunks to maintain context continuity\n",
    "- Overlap prevents important information from being split across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split 40 documents into 218 chunks\n",
      "\n",
      "Average chunk size: 904 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 1008 chars):\n",
      "Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, W...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 2 (length: 1023 chars):\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained pa...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 3 (length: 987 chars):\n",
      "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
      "seq2seq baseline.\n",
      "1 Introduction\n",
      "Pre-trained neural language models have been shown to learn a substantial amount of ...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter with recommended settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximum characters per chunk (roughly 200-250 tokens)\n",
    "    chunk_overlap=128,      # Characters overlap between chunks (maintains context)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "# This creates smaller, manageable pieces while preserving semantic meaning\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display splitting results\n",
    "print(f\"✓ Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Preview a few chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings are vector representations of text that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "**Example**: \n",
    "- \"dog\" and \"puppy\" → similar vectors (close in vector space)\n",
    "- \"dog\" and \"spaceship\" → different vectors (far apart)\n",
    "\n",
    "### OpenAI text-embedding-3-small\n",
    "- **Dimensions**: 1536 (each text becomes a 1536-dimensional vector)\n",
    "- **Cost**: $0.00002 per 1,000 tokens (very affordable)\n",
    "- **Performance**: 62.3% on MTEB benchmark\n",
    "- **Speed**: Fast and efficient\n",
    "\n",
    "**Alternative**: `text-embedding-3-large` for higher quality (64.6% MTEB) at higher cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings model initialized: text-embedding-3-small\n",
      "✓ Embedding dimension: 1536\n",
      "✓ Sample embedding (first 10 values): [0.020370882004499435, -0.0031641265377402306, -0.0005454652709886432, 0.0045827641151845455, -0.015004359185695648, -0.034060992300510406, 0.0176328606903553, 0.01959054544568062, 0.0013125392142683268, 0.00596546521410346]\n",
      "\n",
      "ℹ️  Each chunk will be converted to a 1536-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"✓ Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"✓ Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"✓ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\nℹ️  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Vector Store (FAISS)\n",
    "\n",
    "### What is a Vector Store?\n",
    "A vector store (or vector database) stores embeddings and enables fast similarity search.\n",
    "\n",
    "### FAISS (Facebook AI Similarity Search)\n",
    "- **Fast**: Optimized for billion-scale vector search\n",
    "- **Local**: Runs on your machine, no cloud dependency\n",
    "- **Efficient**: Uses advanced indexing algorithms\n",
    "\n",
    "### How Similarity Search Works:\n",
    "1. Convert query to embedding vector\n",
    "2. Find vectors in the database most similar to query vector (using cosine similarity or Euclidean distance)\n",
    "3. Return the corresponding text chunks\n",
    "\n",
    "**This cell will:**\n",
    "1. Convert all chunks to embeddings (may take a minute for large documents)\n",
    "2. Build a FAISS index\n",
    "3. Save to disk for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index from 218 chunks...\n",
      "This may take a minute depending on the number of chunks...\n",
      "✓ FAISS vector store created successfully!\n",
      "✓ Indexed 218 document chunks\n",
      "✓ Vector store saved to './faiss_index'\n",
      "\n",
      "ℹ️  You can reload this index later using: FAISS.load_local('./faiss_index', embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=embeddings   # OpenAI embedding model\n",
    ")\n",
    "\n",
    "print(f\"✓ FAISS vector store created successfully!\")\n",
    "print(f\"✓ Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"✓ Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\nℹ️  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Saved Vector Store (Optional)\n",
    "\n",
    "If you've already created a vector store, you can load it instead of recreating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded existing vector store from './faiss_index'\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to load an existing vector store instead of creating a new one\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore = FAISS.load_local(\n",
    "    vectorstore_path, \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    ")\n",
    "print(f\"✓ Loaded existing vector store from '{vectorstore_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Feeding all relevant documents directly into LLMs can lead\n",
      "to information overload, diluting the focus on key details with\n",
      "irrelevant content.To mitig...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Table I.\n",
      "B. Indexing Optimization\n",
      "In the Indexing phase, documents will be processed, seg-\n",
      "mented, and transformed into Embeddings to be stored in a\n",
      "v...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: caused by block extraction issues.\n",
      "Knowledge Graph index . Utilize KG in constructing the\n",
      "hierarchical structure of documents contributes to maintaini...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: theless, these approaches still cannot strike a balance between\n",
      "semantic completeness and context length. Therefore, methods\n",
      "like Small2Big have been ...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity for search\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "# Note: In LangChain 1.0+, use .invoke() instead of .get_relevant_documents()\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)  # LangChain 1.0+ method\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuring the Language Model (LLM)\n",
    "\n",
    "### LLM Selection\n",
    "The LLM generates the final answer based on retrieved context.\n",
    "\n",
    "### Available Models:\n",
    "1. **gpt-4-turbo-2025-04-09**: Most capable, best quality, slower, more expensive\n",
    "2. **gpt-4o**: Fast GPT-4 level performance, good balance\n",
    "3. **gpt-3.5-turbo**: Fast and cheap, good for simpler queries\n",
    "\n",
    "### Temperature:\n",
    "- **0**: Deterministic, focused answers (recommended for factual Q&A)\n",
    "- **0.7**: More creative, varied responses\n",
    "- **1.0**: Most creative, less predictable\n",
    "\n",
    "### Max Tokens:\n",
    "Controls the maximum length of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM configured successfully\n",
      "  - Model: gpt-4-turbo-2024-04-09\n",
      "  - Temperature: 0 (deterministic)\n",
      "  - Max tokens: 2000\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",  # Choose your model\n",
    "      # Alternative options:\n",
    "      # model=\"gpt-4o\",           # Faster GPT-4 performance, good \n",
    "      # balance,\n",
    "      # model=\"gpt-3.5-turbo\",    # Faster and cheaper option\n",
    "\n",
    "      temperature=0,         # 0 = deterministic, factual responses (recommended for Q&A)\n",
    "      max_tokens=2000,       # Maximum length of response\n",
    "  )\n",
    "\n",
    "print(\"✓ LLM configured successfully\")\n",
    "print(f\"  - Model: gpt-4-turbo-2024-04-09\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "print(f\"  - Max tokens: 2000\")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready to answer questions!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")\n",
    "\n",
    "#   📝 Explanation of Parameters:\n",
    "\n",
    "#   Model Selection:\n",
    "\n",
    "#   # Option 1: Best quality (slower, more expensive)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "#   # Option 2: Fast GPT-4 performance (balanced)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "#   # Option 3: Fast and cheap (good for testing)\n",
    "#   llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "#   Temperature:\n",
    "\n",
    "#   temperature=0    # Deterministic, focused (best for factual Q&A)\n",
    "#   temperature=0.7  # More creative, varied responses\n",
    "#   temperature=1.0  # Most creative, less predictable\n",
    "\n",
    "#   Max Tokens:\n",
    "\n",
    "#   max_tokens=2000  # Controls maximum response length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the RAG Chain (LangChain 1.0+ LCEL)\n",
    "\n",
    "### What is a RAG Chain?\n",
    "The RAG chain combines retrieval and generation into a single workflow:\n",
    "1. User asks a question\n",
    "2. Retriever finds relevant documents\n",
    "3. Documents are formatted as context\n",
    "4. LLM generates answer using the context\n",
    "\n",
    "### LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "LangChain 1.0+ uses LCEL, a declarative way to build chains using the pipe operator `|`.\n",
    "\n",
    "**Benefits:**\n",
    "- More intuitive and readable\n",
    "- Better streaming support\n",
    "- Easier to debug and modify\n",
    "- Type-safe and composable\n",
    "\n",
    "**Components:**\n",
    "- **RunnablePassthrough**: Passes input through unchanged\n",
    "- **Pipe operator (|)**: Chains components together\n",
    "- **StrOutputParser**: Converts LLM output to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Feeding all relevant documents directly into LLMs can lead\n",
      "to information overload, diluting the focus on key details with\n",
      "irrelevant content.To mitig...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Table I.\n",
      "B. Indexing Optimization\n",
      "In the Indexing phase, documents will be processed, seg-\n",
      "mented, and transformed into Embeddings to be stored in a\n",
      "v...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: caused by block extraction issues.\n",
      "Knowledge Graph index . Utilize KG in constructing the\n",
      "hierarchical structure of documents contributes to maintaini...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: theless, these approaches still cannot strike a balance between\n",
      "semantic completeness and context length. Therefore, methods\n",
      "like Small2Big have been ...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create the Retriever \n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "      search_type=\"similarity\",    # Use cosine similarity for search\n",
    "      search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    "  )\n",
    "\n",
    "print(\"✓ Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "  # Test the retriever with a sample query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG chain created successfully using LangChain 1.0+ LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question are formatted with prompt template\n",
      "  5. LLM generates answer based on context\n",
      "  6. Answer is parsed and returned to user\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the RAG system\n",
    "# This tells the LLM how to use the retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "# This uses the pipe operator (|) to chain components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain created successfully using LangChain 1.0+ LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question are formatted with prompt template\")\n",
    "print(\"  5. LLM generates answer based on context\")\n",
    "print(\"  6. Answer is parsed and returned to user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or subject of this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The main topic of the document is the optimization and improvement of Retrieval-Augmented Generation (RAG) systems, focusing on indexing strategies, chunking methods, and the modular RAG architecture to enhance information retrieval and processing in large language models (LLMs).\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "  Content: Table I.\n",
      "B. Indexing Optimization\n",
      "In the Indexing phase, documents will be processed, seg-\n",
      "mented, and transformed into Embeddings to be stored in a\n",
      "vector database. The quality of index construction ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}\n",
      "  Content: Feeding all relevant documents directly into LLMs can lead\n",
      "to information overload, diluting the focus on key details with\n",
      "irrelevant content.To mitigate this, post-retrieval efforts con-\n",
      "centrate on ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Source: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}\n",
      "  Content: Document 1: his works are considered classics of American\n",
      "literature ... His wartime experiences formed the basis for his novel\n",
      "”A Farewell to Arms”(1929) ...\n",
      "Document 2: ... artists of the 1920s ”Los...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}\n",
      "  Content: caused by block extraction issues.\n",
      "Knowledge Graph index . Utilize KG in constructing the\n",
      "hierarchical structure of documents contributes to maintaining\n",
      "consistency. It delineates the connections betw...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question about the document\n",
    "query1 = \"What is the main topic or subject of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# With LangChain 1.0+, we invoke the chain with the question directly\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# To see which documents were retrieved, we can call the retriever separately\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key points from this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses various aspects of document indexing, retrieval, and optimization in the context of large language models (LLMs) and Retrieval-Augmented Generation (RAG) systems:\n",
      "\n",
      "1. **Indexing Optimization**: Documents are processed into embeddings and stored in a vector database. The chunking strategy involves splitting documents into fixed-size chunks, which affects the balance between context capture and noise. Recursive splits and sliding window methods are used to optimize retrieval.\n",
      "\n",
      "2. **Post-Retrieval Processing**: To avoid information overload when feeding documents into LLMs, post-retrieval efforts focus on selecting essential information and shortening the context.\n",
      "\n",
      "3. **Modular RAG Architecture**: This advanced architecture offers enhanced adaptability and versatility by incorporating strategies like similarity search modules and fine-tuning of retrievers. It supports both sequential processing and integrated training.\n",
      "\n",
      "4. **Metadata Attachments**: Enhancing chunks with metadata such as page numbers or file names helps in filtering and time-aware retrieval, ensuring the relevance and freshness of the information.\n",
      "\n",
      "These points highlight the ongoing efforts to refine and optimize the interaction between document retrieval systems and large language models to improve the accuracy and efficiency of information retrieval and generation.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key points from this document?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please you Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about Advanced RAG introduces specific improvements to?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Advanced RAG introduces specific improvements around pre-retrieval and post-retrieval optimization strategies. These strategies focus on enhancing the indexing structure and the original query, which includes enhancing data granularity, optimizing index structures, and adding metadata.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What specific details are mentioned about Advanced RAG introduces specific improvements to?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about Advanced RAG?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The retrieved context does not provide specific details about Advanced RAG. It mentions the development of Advanced RAG and Modular RAG as a response to specific shortcomings in Naive RAG, but it does not elaborate on what Advanced RAG specifically entails or its features.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What specific details are mentioned about Advanced RAG?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# response3 = rag_chain.invoke({\"input\": custom_query})\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "# print(response3[\"answer\"])\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
